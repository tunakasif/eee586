\section{Related Work}\label{sec:related_work}
\subsection{Text Classification}\label{sec:related:TC}
In~\autocite{li20tc}, \emph{text classification} (text categorization) is defined as the ``procedure of designating predefined labels for the text''. The task is to assign labels or tags to the text based knowledge, i.e., textual units such as sentences, paragraphs and documents, where the labels are usually defined by humans, but can also be defined by the machine. This task is a fundamental part of Natural Language Processing (NLP), and it is significant to its applications such as sentiment analysis, question answering, text summarization, etc.. Text classification task can be partitioned into five phases as preprocessing, feature extraction, dimensionality reduction (optional), classifier selection and evaluation:

\subsubsection{Preprocessing}
Text preprocessing is a crucial prerequisite for a successful feature extraction, and summarized in~\autocite{kowsari19tc} as follows. The input of the text classification frameworks consists of raw text data, which are in the form of a sequence of sentences. In this step, ``cleaning'' of the text datasets is performed to transform the data into a form that is suitable for feature extraction. The cleaning process is usually performed by tokenization, capitalization, slang and abbreviation handling, noise removal, spelling correction, stemming and lemmatization.

\subsubsection{Feature Extraction}
After preprocessing step, another crucial step, feature extraction step is necessary. In~\autocite{kowsari19tc}, this step explained as follows. Two common methods of text based feature extraction are weighted word and word embedding techniques. In the weighted word aspect, we have old techniques like bag-of-words and term frequency-inverse document frequency (TF-IDF). In the relatively recent aspect, we have the word embedding techniques like \emph{word2vec}, \emph{GloVe}, \emph{FastText}, etc.

\subsubsection{Dimensionality Reduction}
The dimensionality reduction is an optional step of a text classification task, but based on the size of the dataset, it may be a must to have a computable result. In this aspect of the task, we try to reduce the dimensionality of the feature space while preserving the information of the original features space. Some possible dimensionality reduction techniques provided in~\autocite{kowsari19tc} include (principal / independent) component analysis, linear discriminant analysis, non-negative matrix factorization, random projection, autoencoder and stochastic neighbor embedding.

\subsubsection{Classifier Selection}
As it is stated in~\autocite{li20tc}, selecting the optimal classifier is the most important aspect of a text classification task. Currently we have both traditional and deep learning oriented classifiers. The traditional classifiers are based on the statistical analysis of the training data, and the deep learning classifiers are based on the neural networks. The main distinction between the traditional and deep learning based approaches can be described as follows: Good feature extraction methodology is crucial for the traditional classifiers. They obtain sample features by artificial methods and then make classifications based on these features. Hence, the performance of the traditional classifiers are mainly restricted by feature extraction. On the other hand, by making feature mapping via nonlinear transformations a part of the learning process, deep learning based classifier selection can integrate feature extraction aspect into the model fitting process.

Examples of both traditional and deep learning based approaches are provided in~\autocite{kowsari19tc,li20tc,minaee20tc}: Some traditional classifiers are logistic regression, (kernel) support vector machine, Naive Bayes, \(k\)-nearest neighbors, decision tree, random forest, etc. On the other hand, the deep learning classifiers are usually based on the neural networks, such as deep feed forward neural networks, convolutional neural networks (CNNs), recurrent neural networks (RNNs), lately attention and transformer based models such as BERT~\autocite{devlin18bert} variations and fine tuning pre-trained language models~\autocite{howard18tc}, and finally what we will focus on, graph neural network (GNN) based models.

\subsubsection{Evaluation}
Evaluation is step that we understand how the our model performs under the given text classification task. As it is provided in~\autocite{li20tc,minaee20tc}, there are several evaluation metrics that can be used to evaluate the performance of a supervised technique. The most common metrics are accuracy, \(F_\beta \)-score, micro/macro-averaging. Although we also have metrics like Matthews correlation coefficient and receiver operating characteristics (ROC). In order to evaluate the performance of our model, based on the provided techniques, we need to use labeled data, i.e., we need benchmark datasets like GLUE~\autocite{wang18glue}, TweetEval~\autocite{tweeteval}, among others.

\subsection{Graph Neural Networks}\label{sec:related:GNN}
In recent years, deep learning based solutions surpassed any approach on machine learning tasks such as image classification, video processing, speech recognition and natural language processing. In these tasks, the underlying data are usually represented in the Euclidean domain. However, each day the amount of non-Euclidean data increases, which are represented as represented by graphs to capture the underlying the complex relationships and interdependency between objects. Therefore, a need for deep learning methods that can manage graph structured data has emerged. In this context, the graph neural networks (GNNs) are born, and many of the deep learning approaches are converted to graph domain such as recurrent GNNs, convolutional GNNs (ConvGNNs) or graph convolutional networks (GCNs), graph autoencoder (GAE), graph reinforcement learning (GRL), graph adversarial methods and spatial-temporal GNNs, as summarized in~\autocite{zhou20gnn,wu21gnn,zhang18dlongraphs,sun18adversarial}.

\subsubsection{Reasons to use GNNs}
Hidden patterns residing under Euclidean data can be effectively obtained by traditional deep learning techniques. However, the increasing number of applications based on a non-Euclidean data structure enforces the necessity of graph based solutions. In this aspect, the following examples can be used to illustrate the benefits of having a graph based deep learning framework~\autocite{wu21gnn}:
\begin{itemize}
    \item In e-commerce, highly accurate recommendation system can be achieved by using graph based deep learning techniques, since the interactions between users and products are a textbook example of graph structured data.
    \item For drug discovery in chemistry, we need to obtain the bioactivity of the molecules, where the molecules are modeled as graphs.
    \item Categorization of articles in a citation network, where the articles are linked to each other via ``citationships'', i.e., forming a graph structure.
\end{itemize}

\subsubsection{Challenges to use GNNs}
In order to have a graph domain deep learning framework, we need to overcome several challenges imposed by the complexity of the graph data. Due to the nature of graphs, when they are compared with Euclidean data, they can be irregular, they can have unordered nodes with different number of neighbors. Hence, many basic operations defined in Euclidean domain can be challenging to apply to the graph domain, e.g., convolution operation. In addition, one of the fundamental assumption we have in the existing machine learning algorithms is that the instances are independent of each other, although this assumption is not valid for graph data since each instance (node) is related to others by links of various types~\autocite{wu21gnn}. Some of the main challenges can be categorized as follows~\autocite{zhang18dlongraphs}:

\paragraph{Irregular structures of graphs} We have the \emph{geometric deep learning problem} which is the inability to define basic operations like convolution and pooling in the graph domain, which are essential aspects of traditional CNNs.

\paragraph{Heterogeneity and diversity of graphs}
We have many different properties that a single graph can have: graphs can be homogenous or heterogeneous, they can be weighted or unweighted, they can be directed or undirected, and they can be signed or unsigned. Furthermore, the tasks may consist of node-level problems such as node classification, link prediction or they can consist of graph-level problems such as graph classification or graph generation. Therefore, we need a spectrum of architectures to tackle all these problems one-by-one.

\paragraph{Large-scale graphs}
As in the case of e-commerce and social networks, graph structured data can have a large number of nodes and edges. However, we still need appropriate algorithms to work on the graph structure without increasing the computational and time complexity too much.

\paragraph{Incorporating interdisciplinary knowledge}
Graph structured data sometimes traces back to other disciplines such as biology, chemistry and social sciences. The interdisciplinary nature helps to leverage domain knowledge to solve specific problems, but it can also complicate model designs. For the case of molecular graph generation, the chemical constraints and the generation's objective function are often non-differentiable. Hence, gradient based training methods are out of the picture.

\subsubsection{Ways to use GNNs}
In~\autocite{zhou20gnn}, a general design to pipeline of GNNs is proposed. The following steps are necessary to obtain a graph-based deep learning framework:
\paragraph{Finding a graph structure}
Based on the application in hand, we need to find out the underlying graph structure. There are two possibilities. First one is that we have an explicit graph structure, in the application such as social network, physical system or knowledge graph. The other possibility is that the underlying graph is implicit, and we need to build the graph from the task, such as obtaining a fully-connected ``word'' graph for text or obtaining a scene graph for an image. Then, we can obtain an optimal GNN model for the the graph we obtained either from explicit information or from the task.

\paragraph{Design a loss function}
Based on the task in hand and the training setting, a loss function needs to be determined, the loss function can be node-level, edge-level or graph-level, depending on the training setting of supervised, semi-supervised or unsupervised learning.

\paragraph{Build model using computational modules}
Finally, we need computational modules to build and train our model. Based on the definition provided in~\autocite{zhou20gnn}, we need a module to conduct convolution and recurrent operations to propagate information between nodes to capture the underlying feature and topological information. We need a sampling module, and we need a pooling module. With the combination of these modules a typical architecture of GNN model can be built.

\subsection{Text Classification with GNN}\label{sec:related:tc_with_gnn}
Some of the earliest success achieved on deep learning with graphs relied on finding proper ways to embed nodes into vectors using an encoder function~\autocite{velickovic21gnn}. One question arises from that definition is the \emph{``What is a good representation?''}. We want these nodes embeddings to preserve interesting structures of the graphs. There are unsupervised graph representations learning algorithms like \emph{node2vec}~\autocite{node2vec}, \emph{DeepWalk}~\autocite{deepwalk2014} and LINE~\autocite{line2015} which are trained prior to graph neural networks. These algorithms aimed to learn representative embeddings for nodes to preserve interesting structures of the graphs

Aforementioned algorithms inherently capture local similarities. Further studies find that Convolutional Graph Neural Networks (ConvGNNs) summarizes local patches of the graphs and shows that neighboring nodes tends to highly overlap~\autocite{velickovic21gnn}. Therefore, a ConvGNNs enforce similar features for neighboring nodes by its nature without needing pre-training for node embeddings. This phenomenon was also mentioned in Text Graph Convolutional Network (Text GCN)~\autocite{yao18graph}. Results of this paper on multiple benchmark data sets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings jointly.

In~\autocite{yao18graph} they evaluate Text GCN on two experimental tasks. First they seek the answer of whether their model achieve satisfactory results in text classification, even with limited labeled data and then they test whether their model learn predictive word and document embeddings. They compare their method with several state-of-art models. The suggested Text GCN may produce high text classification results and train predictive document and word embeddings, according to the experimental results. However, a major limitation of this study is that the GCN model is inherently transductive~\autocite{yao18graph}, in which test document nodes (without labels) are included in GCN training. Thus Text GCN could not quickly generate embeddings and make prediction for unseen test documents.

To improve the weakness of Text GCN in text classification task,~\autocite{velickovic18graph} and~\autocite{fastGCN2018} was mentioned in future work section of~\autocite{yao18graph}. In~\autocite{velickovic18graph} a novel algorithm Graph Attention Networks (GATs) was proposed. It was mentioned in~\autocite{velickovic18graph} that GATs are new convolutional-style neural networks that operate on graph-structured data and use masked self-attentional layers. The graph attentional layer used in these networks is computationally efficient. Attentional layers do not require expensive matrix operations and they are parallelizable across all nodes in the graph. This structure allows for (implicitly) assigning different importance to different nodes within a neighborhood while dealing with different sized neighborhoods, and does not require knowing the entire graph structure. The experimental results yields that their attention-based models outperformed or matched state-of-the-art performance in four well-known node classification benchmarks, both transductive and inductive tasks~\autocite{velickovic18graph}.

%Fast GCN
Fast Graph Convolutional Neural Network~\autocite{fastGCN2018} was also mentioned in the future work section of~\autocite{yao18graph}. In~\autocite{fastGCN2018} it was mentioned that, GCN in~\autocite{kipf17semisupervised} represented as a useful graph model for semi-supervised learning. This model was created with the intention of being taught with both training and test data. Furthermore, for training with large, dense graphs, the recursive neighborhood expansion across layers faces time and memory issues.~\autocite{fastGCN2018} interpret graph convolutions as integral transforms of embedding functions under probability measures to relax the condition of simultaneous availability of test data. As a result of this interpretation, Monte Carlo techniques may be used to consistently estimate the integrals, leading to a batched training scheme like FastGCN, which is proposed~\autocite{fastGCN2018}.

% Simplifying gcns
After further development on top of Text GCN, Simplifying Graph Convolutional Networks~\autocite{simplfyingGCN2019} was proposed to overcome unnecessary complexity and redundant computation in the previous work of Fast GCN and GATs. GCNs and their variants have received a lot of attention and have become the defacto methods for learning graph representations. GCNs are primarily inspired by modern deep learning methodologies, and as a result, they may inherit extra complexity and redundant processing. In~\autocite{simplfyingGCN2019} they eliminate the unnecessary complexity in this paper by reducing non-linearities one by one and collapsing weight matrices between layers. The resulting linear model is theoretically analyzed and shown to correspond to a fixed low-pass filter followed by a linear classifier in. The test results in~\autocite{simplfyingGCN2019} shows that these simplifications have no negative influence on accuracy in a wide range of downstream applications. Furthermore, the resulting model scales to bigger datasets, is intuitively interpretable, and outperforms FastGCN by up to two orders of magnitude.

\subsection{BERT-GNN Architectures}\label{sec:related:bertgnn}
After coming up with the idea of combining BERT models with GNN based models to increase text classification performance, we have encountered a few similar approaches that tries to accomplish a similar task. The first significant approach in combining BERT and GNN architectures is the \emph{VGCN-BERT} proposed in~\autocite{zhibin2020vgcn}. However, in this approach the authors generate a vocabulary graph to produce word embeddings using the GCN architecture, then they supply these embeddings as input to the BERT architecture. This approach is different than what we are trying to achieve, since we are proposing aggregation of these embeddings since they both embody different aspects of the information.

In addition to this approach, in~\autocite{yang2021bertenhanced}, \emph{BEGNN} model is proposed, which aggregates graph embeddings with BERT embeddings similar to out approach, although they are generating graph embeddings for each document separately, so graph embeddings that are used in BEGNN are limited to the global information of that specific document. This approach can be limited, since with graph embeddings we try to convey the global and structural information of the texts, and we propose an extension to this manner by generating graph embeddings using whole training set that can embody structural and global information in a more generic sense.

After our project proposal is finalized, there has been several publications regarding the text classification with combination of GNN and BERT architectures~\autocite{lin2022bertgcn,she2022bertgcnattention,zeng2022boosting}. In these publications, there are some similar approaches to our project, although we still have some differences in our methods, when it is compared to these fresh publications.
