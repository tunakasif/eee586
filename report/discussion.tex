\section{Discussion \& Conclusion}\label{sec:discussion}

\subsection{Obtained Results}
Unfortunately, obtained results did not surpass the SOTA algorithms that mentioned in \cref{results:sota}. However, we are aware of the problems that occurred during our implementation. These issues are mentioned in the \cref{issues}. Our model still can pass some of the other algorithms. 

\subsection{Issues}\label{issues}
During our implementation of the algorithm, we have discovered several problems and solved most of them. The very first problem that we faced with was the instability of the 20NG dataset in different websites. This dataset is used in both supervised and semi-supervised manner in the literature. The graph convolutional network proposed in \cite{kipf17semisupervised} is for semi-supervised learning problem. \cite{yao18graph} also proposed for only semi-supervised learning problem. \cite{yao18graph} treats 20NG dataset as a semi-supervised learning problem. In that version of 20NG, they do not use labels of every data instance. However, in  \href{https://huggingface.co/datasets/SetFit/20_newsgroups}{huggingface.co}, all of the data are labeled. In our structure, we tried to treat problem in inductive manner since we know all of the labels. However, we used the GCN architecture of the \href{https://pytorch-geometric.readthedocs.io/en/latest/}{PyTorch Geometric} which only accepts the semi-supervised learning tasks and it asks to pre-define the labeled data to do back propagation only on these labeled data. Since we know all of the labels, we pre-define the labeled data as whole training set.

\subsection{Future Directions}
To solve the mentioned problems in \cref{issues}, we can treat whole problem in inductive manner and use more appropriate GNN structures. One of the candidate for inductive learning problem is \cite{fastGCN2018}. It solves the problem totally in inductive manner which also makes computation faster. Another candidate architecture for the GNN side is \cite{guo19attention}. It also enables us to learn more interesting features since it uses attention mechanism rather simple matrix multiplications in \cref{eq2}.