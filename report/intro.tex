\section{Introduction}
In this project, we have scrutinize the classic natural language processing task of text classification along with the trending approach of graph neural networks (GNNs). In this context, we present a text classification approach that combines the BERT models, that provide semantic and contextual information, with the GCN models, that provide structural and global information. In this project, we first provide a comprehensive related work analysis in~\cref{sec:related_work}. Then, in~\cref{sec:methods}, we present our proposed method of combining BERT based approaches with GNN based approaches in the context of text classification. Then, we present our results and compare them with the baselines in~\cref{sec:results}. Finally, we provide a discussion and possible future directions in~\cref{sec:discussion}.

We have divided the related work analysis into four parts in~\cref{sec:related_work}. First, we provide an overview regarding the traditional text classification (\cref{sec:related:TC}) and we provide an overview to graph neural networks (\cref{sec:related:GNN}). Both of these overviews are provided based on several survey papers for text classification~\autocite{kowsari19tc,li20tc,minaee20tc} and for graph neural networks~\autocite{zhou20gnn,wu21gnn,zhang18dlongraphs,sun18adversarial}. In~\cref{sec:related:TC}, we provide a basic definition for text classification, then we disintegrate text classification process to five steps to analyse the overall process, and we point out where the integration of graph neural networks can be made. In~\cref{sec:related:GNN}, we provide an introduction to graph neural networks. Then, we analyse why we need graph neural networks, we provide several challenges on the route of obtaining graph based deep learning frameworks, and finally we provide bare-minimum steps in order to obtain a graph neural network based architecture, based on the aforementioned surveys, in this subsection. After this step, we are at a stage that we have provided background information both on the preliminary aspects of the overall term project, which are text classification and graph neural networks. 

In~\cref{sec:related:tc_with_gnn}, we provide previous work directly related to our own topic of graph neural networks related to solution of the classical natural language processing task of text classification. \cref{sec:related:tc_with_gnn} of the survey aims to investigate the related and previous work in the text classification with graph neural network field possibly with a historical order. All the papers mentioned in this section related with each other by references. Surprisingly, this structure can also be viewed as graph where each paper is node and and edges of the graphs as citation. There are also studies on this topic to predict structured citation trend~\autocite{citationGNN}. Most of the papers in the~\cref{sec:related:tc_with_gnn} did not directly proposed to solve text classification task. However, nearly all of the papers presented~\cref{sec:related:tc_with_gnn} used text classification bench mark data sets to evaluate model performance. Finally, in~\cref{sec:related:bertgnn}, we present the similar approaches that combine BERT architecture with GNNs for text classification~\autocite{zhibin2020vgcn,yang2021bertenhanced,lin2022bertgcn,she2022bertgcnattention,zeng2022boosting}. 

In~\cref{sec:methods}, we present our approach, and give further details that how we implemented the proposed method, by also comparing with the related work discussed in~\cref{sec:related_work}. We provide how we are generating both BERT \& GCN embeddings and how we are combining these embeddings to generate a better representation for documents to obtain a text classification model. Then, we present the results of the proposed model in~\cref{sec:results}, and compare them with the baselines discussed in~\cref{sec:related_work}.

Finally, in~\cref{sec:discussion}, we provide a discussion that we comment on the obtained results, issues that we had encountered, and possible future directions for the project.
