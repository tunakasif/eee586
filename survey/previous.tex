\section{Previous Work}
\subsection{Text Classification}\label{sec:TC}
In~\autocite{li20tc}, \emph{text classification} (text categorization) is defined as the ``procedure of designating predefined labels for the text''. The task is to assign labels or tags to the text based knowledge, i.e., textual units such as sentences, paragraphs and documents, where the labels are usually defined by humans, but can also be defined by the machine. This task is a fundamental part of Natural Language Processing (NLP), and it is significant to its applications such as sentiment analysis, question answering, text summarization, etc.. Text classification task can be partitioned into five phases as preprocessing, feature extraction, dimensionality reduction (optional), classifier selection and evaluation:

\subsubsection{Preprocessing}
Text preprocessing is a crucial prerequisite for a successful feature extraction, and summarized in~\autocite{kowsari19tc} as follows. The input of the text classification frameworks consists of raw text data, which are in the form of a sequence of sentences. In this step, ``cleaning'' of the text datasets is performed to transform the data into a form that is suitable for feature extraction. The cleaning process is usually performed by tokenization, capitalization, slang and abbreviation handling, noise removal, spelling correction, stemming and lemmatization.

\subsubsection{Feature Extraction}
After preprocessing step, another crucial step, feature extraction step is necessary. In~\autocite{kowsari19tc}, this step explained as follows. Two common methods of text based feature extraction are weighted word and word embedding techniques. In the weighted word aspect, we have old techniques like bag-of-words and term frequency-inverse document frequency (TF-IDF). In the relatively recent aspect, we have the word embedding techniques like \emph{word2vec}, \emph{GloVe}, \emph{FastText}, etc.

\subsubsection{Dimensionality Reduction}
The dimensionality reduction is an optional step of a text classification task, but based on the size of the dataset, it may be a must to have a computable result. In this aspect of the task, we try to reduce the dimensionality of the feature space while preserving the information of the original features space. Some possible dimensionality reduction techniques provided in~\autocite{kowsari19tc} include (principal / independent) component analysis, linear discriminant analysis, non-negative matrix factorization, random projection, autoencoder and stochastic neighbor embedding.

\subsubsection{Classifier Selection}
As it is stated in~\autocite{li20tc}, selecting the optimal classifier is the most important aspect of a text classification task. Currently we have both traditional and deep learning oriented classifiers. The traditional classifiers are based on the statistical analysis of the training data, and the deep learning classifiers are based on the neural networks. The main distinction between the traditional and deep learning based approaches can be described as follows: Good feature extraction methodology is crucial for the traditional classifiers. They obtain sample features by artificial methods and then make classifications based on these features. Hence, the performance of the traditional classifiers are mainly restricted by feature extraction. On the other hand, by making feature mapping via nonlinear transformations a part of the learning process, deep learning based classifier selection can integrate feature extraction aspect into the model fitting process.

Examples of both traditional and deep learning based approaches are provided in~\autocite{kowsari19tc,li20tc,minaee20tc}: Some traditional classifiers are logistic regression, (kernel) support vector machine, Naive Bayes, \(k\)-nearest neighbors, decision tree, random forest, etc. On the other hand, the deep learning classifiers are usually based on the neural networks, such as deep feed forward neural networks, convolutional neural networks (CNNs), recurrent neural networks (RNNs), lately attention and transformer based models such as BERT~\autocite{devlin18bert} variations and fine tuning pre-trained language models~\autocite{howard18tc}, and finally what we will focus on, graph neural network (GNN) based models.

\subsubsection{Evaluation}
Evaluation is step that we understand how the our model performs under the given text classification task. As it is provided in~\autocite{li20tc,minaee20tc}, there are several evaluation metrics that can be used to evaluate the performance of a supervised technique. The most common metrics are accuracy, \(F_\beta \)-score, micro/macro-averaging. Although we also have metrics like Matthews correlation coefficient and receiver operating characteristics (ROC). In order to evaluate the performance of our model, based on the provided techniques, we need to use labeled data, i.e., we need benchmark datasets like GLUE~\autocite{wang18glue}, TweetEval~\autocite{tweeteval}, among others.

\subsection{Graph Neural Networks}\label{sec:GNN}
In recent years, deep learning based solutions surpassed any approach on machine learning tasks such as image classification, video processing, speech recognition and natural language processing. In these tasks, the underlying data are usually represented in the Euclidean domain. However, each day the amount of non-Euclidean data increases, which are represented as represented by graphs to capture the underlying the complex relationships and interdependency between objects. Therefore, a need for deep learning methods that can manage graph structured data has emerged. In this context, the graph neural networks (GNNs) are born, and many of the deep learning approaches are converted to graph domain such as recurrent GNNs, convolutional GNNs (ConvGNNs) or graph convolutional networks (GCNs), graph autoencoders (GAE), graph reinforcement learning (GRL), graph adversarial methods and spatial-temporal GNNs, as summarized in~\autocite{zhou20gnn,wu21gnn,zhang18dlongraphs,sun18adversarial}.

\subsubsection{Reasons to use GNNs}
Hidden patterns residing under Euclidean data can be effectively obtained by traditional deep learning techniques. However, the increasing number of applications based on a non-Euclidean data structure enforces the necessity of graph based solutions. In this aspect, the following examples can be used to illustrate the benefits of having a graph based deep learning framework~\autocite{wu21gnn}:
\begin{itemize}
    \item In e-commerce, highly accurate recommendation system can be achieved by using graph based deep learning techniques, since the interactions between users and products are a textbook example of graph structured data.
    \item For drug discovery in chemistry, we need to obtain the bioactivity of the molecules, where the molecules are modeled as graphs.
    \item Categorization of articles in a citation network, where the articles are linked to each other via ``citationships'', i.e., forming a graph structure.
\end{itemize}

\subsubsection{Challenges to use GNNs}
In order to have a graph domain deep learning framework, we need to overcome several challenges imposed by the complexity of the graph data. Due to the nature of graphs, when they are compared with Euclidean data, they can be irregular, they can have unordered nodes with different number of neighbors. Hence, many basic operations defined in Euclidean domain can be challenging to apply to the graph domain, e.g., convolution operation. In addition, one of the fundamental assumption we have in the existing machine learning algorithms is that the instances are independent of each other, although this assumption is not valid for graph data since each instance (node) is related to others by links of various types~\autocite{wu21gnn}. Some of the main challenges can be categorized as follows~\autocite{zhang18dlongraphs}:

\paragraph{Irregular structures of graphs} We have the \emph{geometric deep learning problem} which is the inability to define basic operations like convolution and pooling in the graph domain, which are essential aspects of traditional CNNs.

\paragraph{Heterogeneity and diversity of graphs}
We have many different properties that a single graph can have: graphs can be homogenous or heterogeneous, they can be weighted or unweighted, they can be directed or undirected, and they can be signed or unsigned. Furthermore, the tasks may consist of node-level problems such as node classification, link prediction or they can consist of graph-level problems such as graph classification or graph generation. Therefore, we need a spectrum of architectures to tackle all these problems one-by-one.

\paragraph{Large-scale graphs}
As in the case of e-commerce and social networks, graph structured data can have a large number of nodes and edges. However, we still need appropriate algorithms to work on the graph structure without increasing the computational and time complexity too much.

\paragraph{Incorporating interdisciplinary knowledge}
Graph structured data sometimes traces back to other disciplines such as biology, chemistry and social sciences. The interdisciplinary nature helps to leverage domain knowledge to solve specific problems, but it can also complicate model designs. For the case of molecular graph generation, the chemical constraints and the generation's objective function are often non-differentiable. Hence, gradient based training methods are out of the picture.

\subsubsection{Ways to use GNNs}
In~\autocite{zhou20gnn}, a general design to pipeline of GNNs is proposed. The following steps are necessary to obtain a graph-based deep learning framework:
\paragraph{Finding a graph structure}
At first, we have to find out the graph structure in the application.
There are usually two scenarios: structural scenarios and non-structural
scenarios. In structural scenarios, the graph structure is explicit in the
applications, such as applications on molecules, physical systems,
knowledge graphs and so on. In non-structural scenarios, graphs are
implicit so that we have to first build the graph from the task, such as
building a fully-connected ``word'' graph for text or building a scene
graph for an image. After we get the graph, the later design process at-
tempts to find an optimal GNN model on this specific graph.

Based on the application in hand, we need to find out the underlying graph structure. There are two possibilities. First one is that we have an explicit graph structure,
