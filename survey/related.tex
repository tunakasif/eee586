\section{Related Work}\label{sec:related} 
Some of the earliest success achieved on deep learning with graphs relied on finding proper ways to embed nodes into vectors using an encoder function~\autocite{velickovic21gnn}. One question arises from that definition is the \emph{"What is a good representation?"}. We want these nodes embeddings to preserve interesting structures of the graphs. There are unsupervised graph representations learning algorithms like \emph{node2vec} \autocite{node2vec}, \emph{DeepWalk} \autocite{deepwalk2014} and LINE~\autocite{line2015} which are trained prior to graph neural networks. These algorithms aimed to learn representative embeddings for nodes to preserve interesting structures of the graphs 

Aforementioned algorithms inherently capture local similarities. Further studies find that Convolutional Graph Neural Networks (ConvGNNs) summarises local patches of the graphs and shows that neighbouring nodes tends to highly overlap \autocite{velickovic21gnn}. Therefore, a ConvGNNs enforce similar features for neighbouring nodes by its nature without needing pre-training for node embeddings. This phenomenon was also mentioned in Text Graph Convolutional Network (Text GCN) \autocite{yao18graph}. Results of this paper on multiple benchmark data sets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings jointly. 

In \autocite{yao18graph} they evaluate Text GCN on two experimental tasks. First they seek the answer of whether their model achieve satisfactory results in text classification, even with limited labeled data and then they test whether their model learn predictive word and document embeddings. They compare their method with several state-of-art models. The suggested Text GCN may produce high text classification results and train predictive document and word embeddings, according to the experimental results. However, a major limitation of this study is that the GCN model is inherently transductive \autocite{yao18graph}, in which test document nodes (without labels) are included in GCN training. Thus Text GCN could not quickly generate embeddings and make prediction for unseen test documents.

To improve the weakness of Text GCN in text classification task, \autocite{velickovic18graph} and \autocite{fastGCN2018} was mentioned in future work section of \autocite{yao18graph}. In \autocite{velickovic18graph} a novel algorithm Graph Attention Networks (GATs) was proposed. It was mentioned in \autocite{velickovic18graph} that GATs are new convolutional-style neural networks that operate on graph-structured data and use masked self-attentional layers. The graph attentional layer used in these networks is computationally efficient. Attentional layers do not require expensive matrix operations and they are parallelizable across all nodes in the graph. This structure allows for (implicitly) assigning different importance to different nodes within a neighborhood while dealing with different sized neighborhoods, and does not require knowing the entire graph structure. The experimental results yields that their attention-based models outperformed or matched state-of-the-art performance in four well-known node classification benchmarks, both transductive and inductive tasks \autocite{velickovic18graph}.

%Fast GCN
Fast Graph Convolutional Neural Network \autocite{fastGCN2018} was also mentioned in the future work section of \autocite{yao18graph}. In \autocite{fastGCN2018} it was mentioned that, GCN in \autocite{kipf17semisupervised} represented as a useful graph model for semi-supervised learning. This model was created with the intention of being taught with both training and test data. Furthermore, for training with large, dense graphs, the recursive neighborhood expansion across layers faces time and memory issues. \autocite{fastGCN2018} interpret graph convolutions as integral transforms of embedding functions under probability measures to relax the condition of simultaneous availability of test data. As a result of this interpretation, Monte Carlo techniques may be used to consistently estimate the integrals, leading to a batched training scheme like FastGCN, which is proposed \cite{fastGCN2018}.

%Simplfying gcns
After further development on top of Text GCN, Simplifying Graph Convolutional Networks \cite{simplfyingGCN2019} was proposed to overcome unnecessary complexity and redundant computation in the previous work of Fast GCN and GATs. GCNs and their variants have received a lot of attention and have become the defacto methods for learning graph representations. GCNs are primarily inspired by modern deep learning methodologies, and as a result, they may inherit extra complexity and redundant processing. In \autocite{simplfyingGCN2019} they eliminate the unnecessary complexity in this paper by reducing non-linearities one by one and collapsing weight matrices between layers. The resulting linear model is theoretically analyzed and shown to correspond to a fixed low-pass filter followed by a linear classifier in. The test results in \autocite{simplfyingGCN2019} shows that these simplifications have no negative influence on accuracy in a wide range of downstream applications. Furthermore, the resulting model scales to bigger datasets, is intuitively interpretable, and outperforms FastGCN by up to two orders of magnitude.

